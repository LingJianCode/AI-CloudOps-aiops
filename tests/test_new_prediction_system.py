#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AI-CloudOps-aiops
Author: Bamboo
Email: bamboocloudops@gmail.com
License: Apache 2.0
Description: æ–°é¢„æµ‹ç³»ç»ŸéªŒè¯è„šæœ¬ - å®Œæ•´æµ‹è¯•æ‰€æœ‰é¢„æµ‹åŠŸèƒ½
"""

import asyncio
from datetime import datetime
import json
import logging
from pathlib import Path
import sys
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.config.settings import config  # noqa: E402
from app.models.predict_models import ResourceConstraints  # noqa: E402
from app.services.prediction_service import PredictionService  # noqa: E402

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("test_prediction_system")


class PredictionSystemValidator:
    """é¢„æµ‹ç³»ç»ŸéªŒè¯å™¨"""

    def __init__(self):
        self.service = PredictionService()
        self.results = {}

    async def initialize_service(self):
        """åˆå§‹åŒ–é¢„æµ‹æœåŠ¡"""
        logger.info("ğŸš€ åˆå§‹åŒ–é¢„æµ‹æœåŠ¡...")
        try:
            await self.service.initialize()
            logger.info("âœ… é¢„æµ‹æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False

    async def test_service_health(self):
        """æµ‹è¯•æœåŠ¡å¥åº·çŠ¶æ€"""
        logger.info("ğŸ¥ æµ‹è¯•æœåŠ¡å¥åº·çŠ¶æ€...")

        try:
            # å¥åº·æ£€æŸ¥
            is_healthy = await self.service.health_check()
            logger.info(f"å¥åº·çŠ¶æ€: {'âœ… å¥åº·' if is_healthy else 'âŒ ä¸å¥åº·'}")

            # è·å–è¯¦ç»†å¥åº·ä¿¡æ¯
            health_info = await self.service.get_service_health_info()
            logger.info(f"æœåŠ¡çŠ¶æ€: {health_info.get('service_status')}")
            logger.info(f"æ¨¡å‹çŠ¶æ€: {health_info.get('model_status')}")
            logger.info(
                f"æ”¯æŒçš„é¢„æµ‹ç±»å‹: {health_info.get('supported_prediction_types', [])}"
            )

            self.results["health_check"] = {
                "healthy": is_healthy,
                "service_status": health_info.get("service_status"),
                "model_status": health_info.get("model_status"),
            }

            return is_healthy

        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            self.results["health_check"] = {"error": str(e)}
            return False

    async def test_model_info(self):
        """æµ‹è¯•æ¨¡å‹ä¿¡æ¯è·å–"""
        logger.info("ğŸ“Š è·å–æ¨¡å‹ä¿¡æ¯...")

        try:
            model_info = await self.service.get_model_info()
            logger.info(f"æ¨¡å‹æ€»æ•°: {model_info.get('total_models', 0)}")
            logger.info(f"æ¨¡å‹å·²åŠ è½½: {model_info.get('models_loaded', False)}")

            if "models" in model_info:
                for model in model_info["models"]:
                    logger.info(
                        f"  - {model.get('type', 'unknown')}: {model.get('name', 'unnamed')}"
                    )

            self.results["model_info"] = model_info
            return True

        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}")
            self.results["model_info"] = {"error": str(e)}
            return False

    async def test_qps_prediction(self):
        """æµ‹è¯•QPSé¢„æµ‹"""
        logger.info("ğŸ“ˆ æµ‹è¯•QPSé¢„æµ‹...")

        try:
            result = await self.service.predict_qps(
                current_qps=150.0,
                prediction_hours=12,
                granularity="hour",
                include_confidence=True,
                include_anomaly_detection=True,
                consider_historical_pattern=True,
                target_utilization=0.7,
                sensitivity=0.8,
            )

            logger.info("âœ… QPSé¢„æµ‹æˆåŠŸ")
            logger.info(f"  å½“å‰QPS: {result['current_value']}")
            logger.info(f"  é¢„æµ‹ç‚¹æ•°: {len(result['predicted_data'])}")
            logger.info(
                f"  æ‰©ç¼©å®¹å»ºè®®æ•°: {len(result.get('scaling_recommendations', []))}"
            )
            logger.info(f"  å¼‚å¸¸é¢„æµ‹æ•°: {len(result.get('anomaly_predictions', []))}")

            # æ˜¾ç¤ºå‰3ä¸ªé¢„æµ‹ç‚¹
            for i, pred in enumerate(result["predicted_data"][:3]):
                logger.info(
                    f"  é¢„æµ‹ç‚¹{i + 1}: {pred['predicted_value']:.2f} (ç½®ä¿¡åº¦: {pred.get('confidence_level', 0):.2f})"
                )

            self.results["qps_prediction"] = {
                "success": True,
                "prediction_count": len(result["predicted_data"]),
                "recommendations_count": len(result.get("scaling_recommendations", [])),
                "anomalies_count": len(result.get("anomaly_predictions", [])),
            }

            return True

        except Exception as e:
            logger.error(f"âŒ QPSé¢„æµ‹å¤±è´¥: {str(e)}")
            self.results["qps_prediction"] = {"error": str(e)}
            return False

    async def test_cpu_prediction(self):
        """æµ‹è¯•CPUé¢„æµ‹"""
        logger.info("ğŸ–¥ï¸ æµ‹è¯•CPUé¢„æµ‹...")

        try:
            constraints = ResourceConstraints(
                cpu_cores=8.0,
                memory_gb=32.0,
                max_instances=10,
                min_instances=2,
                cost_per_hour=1.5,
            )

            result = await self.service.predict_cpu_utilization(
                current_cpu_percent=75.5,
                prediction_hours=24,
                granularity="hour",
                resource_constraints=constraints.dict(),
                target_utilization=0.65,
                include_anomaly_detection=True,
            )

            logger.info("âœ… CPUé¢„æµ‹æˆåŠŸ")
            logger.info(f"  å½“å‰CPU: {result['current_value']}%")
            logger.info(f"  é¢„æµ‹ç‚¹æ•°: {len(result['predicted_data'])}")
            logger.info(
                f"  èµ„æºåˆ©ç”¨ç‡é¢„æµ‹æ•°: {len(result.get('resource_utilization', []))}"
            )

            # æˆæœ¬åˆ†æ
            if result.get("cost_analysis"):
                cost = result["cost_analysis"]
                logger.info(
                    f"  å½“å‰æˆæœ¬: ${cost.get('current_hourly_cost', 0):.2f}/å°æ—¶"
                )
                logger.info(
                    f"  é¢„æµ‹æˆæœ¬: ${cost.get('predicted_hourly_cost', 0):.2f}/å°æ—¶"
                )
                logger.info(f"  èŠ‚çœæ½œåŠ›: {cost.get('cost_savings_potential', 0):.1f}%")

            self.results["cpu_prediction"] = {
                "success": True,
                "prediction_count": len(result["predicted_data"]),
                "has_cost_analysis": result.get("cost_analysis") is not None,
            }

            return True

        except Exception as e:
            logger.error(f"âŒ CPUé¢„æµ‹å¤±è´¥: {str(e)}")
            self.results["cpu_prediction"] = {"error": str(e)}
            return False

    async def test_memory_prediction(self):
        """æµ‹è¯•å†…å­˜é¢„æµ‹"""
        logger.info("ğŸ§  æµ‹è¯•å†…å­˜é¢„æµ‹...")

        try:
            result = await self.service.predict_memory_utilization(
                current_memory_percent=68.2,
                prediction_hours=48,
                granularity="hour",
                include_confidence=True,
                target_utilization=0.75,
            )

            logger.info("âœ… å†…å­˜é¢„æµ‹æˆåŠŸ")
            logger.info(f"  å½“å‰å†…å­˜: {result['current_value']}%")
            logger.info(f"  é¢„æµ‹æ—¶é—´èŒƒå›´: {result['prediction_hours']}å°æ—¶")
            logger.info(f"  æ¨¡å¼åˆ†æ: {result.get('pattern_analysis', {})}")

            # è¶‹åŠ¿æ´å¯Ÿ
            insights = result.get("trend_insights", [])
            if insights:
                logger.info("  è¶‹åŠ¿æ´å¯Ÿ:")
                for insight in insights[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ´å¯Ÿ
                    logger.info(f"    - {insight}")

            self.results["memory_prediction"] = {
                "success": True,
                "prediction_count": len(result["predicted_data"]),
                "insights_count": len(insights),
            }

            return True

        except Exception as e:
            logger.error(f"âŒ å†…å­˜é¢„æµ‹å¤±è´¥: {str(e)}")
            self.results["memory_prediction"] = {"error": str(e)}
            return False

    async def test_disk_prediction(self):
        """æµ‹è¯•ç£ç›˜é¢„æµ‹"""
        logger.info("ğŸ’½ æµ‹è¯•ç£ç›˜é¢„æµ‹...")

        try:
            result = await self.service.predict_disk_utilization(
                current_disk_percent=82.5,
                prediction_hours=72,
                granularity="day",
                sensitivity=0.9,
                include_anomaly_detection=True,
            )

            logger.info("âœ… ç£ç›˜é¢„æµ‹æˆåŠŸ")
            logger.info(f"  å½“å‰ç£ç›˜: {result['current_value']}%")
            logger.info(f"  é¢„æµ‹ç²’åº¦: {result['granularity']}")

            # é¢„æµ‹æ‘˜è¦
            summary = result.get("prediction_summary", {})
            if summary:
                logger.info(f"  æœ€å¤§é¢„æµ‹å€¼: {summary.get('max_value', 0):.1f}%")
                logger.info(f"  æœ€å°é¢„æµ‹å€¼: {summary.get('min_value', 0):.1f}%")
                logger.info(f"  å¹³å‡é¢„æµ‹å€¼: {summary.get('avg_value', 0):.1f}%")
                logger.info(f"  è¶‹åŠ¿: {summary.get('trend', 'unknown')}")

            self.results["disk_prediction"] = {
                "success": True,
                "prediction_count": len(result["predicted_data"]),
                "granularity": result["granularity"],
            }

            return True

        except Exception as e:
            logger.error(f"âŒ ç£ç›˜é¢„æµ‹å¤±è´¥: {str(e)}")
            self.results["disk_prediction"] = {"error": str(e)}
            return False

    async def test_validation_errors(self):
        """æµ‹è¯•å‚æ•°éªŒè¯"""
        logger.info("ğŸ” æµ‹è¯•å‚æ•°éªŒè¯...")

        validation_tests = []

        # æµ‹è¯•æ— æ•ˆQPSå€¼
        try:
            await self.service.predict_qps(current_qps=-10.0)
            validation_tests.append(("invalid_qps", False, "æœªæ•è·æ— æ•ˆQPSå€¼"))
        except Exception as e:
            validation_tests.append(
                ("invalid_qps", True, f"æ­£ç¡®æ•è·: {str(e)[:50]}...")
            )

        # æµ‹è¯•æ— æ•ˆåˆ©ç”¨ç‡
        try:
            await self.service.predict_cpu_utilization(current_cpu_percent=150.0)
            validation_tests.append(("invalid_cpu", False, "æœªæ•è·æ— æ•ˆCPUå€¼"))
        except Exception as e:
            validation_tests.append(
                ("invalid_cpu", True, f"æ­£ç¡®æ•è·: {str(e)[:50]}...")
            )

        # æµ‹è¯•æ— æ•ˆé¢„æµ‹æ—¶é•¿
        try:
            await self.service.predict_memory_utilization(
                current_memory_percent=50.0, prediction_hours=0
            )
            validation_tests.append(("invalid_hours", False, "æœªæ•è·æ— æ•ˆæ—¶é•¿"))
        except Exception as e:
            validation_tests.append(
                ("invalid_hours", True, f"æ­£ç¡®æ•è·: {str(e)[:50]}...")
            )

        # è¾“å‡ºéªŒè¯ç»“æœ
        passed_count = sum(1 for _, passed, _ in validation_tests if passed)
        logger.info(f"å‚æ•°éªŒè¯æµ‹è¯•: {passed_count}/{len(validation_tests)} é€šè¿‡")

        for test_name, passed, message in validation_tests:
            status = "âœ…" if passed else "âŒ"
            logger.info(f"  {status} {test_name}: {message}")

        self.results["validation_tests"] = {
            "passed": passed_count,
            "total": len(validation_tests),
            "details": validation_tests,
        }

        return passed_count == len(validation_tests)

    async def test_configuration_integration(self):
        """æµ‹è¯•é…ç½®é›†æˆ"""
        logger.info("âš™ï¸ æµ‹è¯•é…ç½®é›†æˆ...")

        try:
            # æ£€æŸ¥é…ç½®æ–‡ä»¶è¯»å–
            pred_config = config.prediction
            logger.info(f"æ¨¡å‹åŸºç¡€è·¯å¾„: {pred_config.model_base_path}")
            logger.info(f"é»˜è®¤é¢„æµ‹æ—¶é•¿: {pred_config.default_prediction_hours}å°æ—¶")
            logger.info(f"æœ€å¤§é¢„æµ‹æ—¶é•¿: {pred_config.max_prediction_hours}å°æ—¶")
            logger.info(f"é»˜è®¤ç›®æ ‡åˆ©ç”¨ç‡: {pred_config.default_target_utilization}")

            # æ£€æŸ¥æ‰©ç¼©å®¹é˜ˆå€¼é…ç½®
            thresholds = pred_config.scaling_thresholds
            logger.info(
                f"QPSæ‰©å®¹é˜ˆå€¼: {thresholds.get('qps', {}).get('scale_up', 'N/A')}"
            )
            logger.info(
                f"CPUæ‰©å®¹é˜ˆå€¼: {thresholds.get('cpu', {}).get('scale_up', 'N/A')}"
            )

            # æ£€æŸ¥å†·å´æ—¶é—´é…ç½®
            cooldown = pred_config.cooldown_periods
            logger.info(f"æ‰©å®¹å†·å´æ—¶é—´: {cooldown.get('scale_up', 'N/A')}åˆ†é’Ÿ")
            logger.info(f"ç¼©å®¹å†·å´æ—¶é—´: {cooldown.get('scale_down', 'N/A')}åˆ†é’Ÿ")

            self.results["configuration"] = {
                "model_base_path": pred_config.model_base_path,
                "scaling_thresholds_configured": len(thresholds) > 0,
                "cooldown_configured": len(cooldown) > 0,
            }

            logger.info("âœ… é…ç½®é›†æˆæµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"âŒ é…ç½®é›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")
            self.results["configuration"] = {"error": str(e)}
            return False

    async def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")

        report = {
            "test_time": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.results),
                "passed_tests": sum(
                    1
                    for r in self.results.values()
                    if isinstance(r, dict)
                    and r.get("success", False)
                    and "error" not in r
                ),
                "failed_tests": sum(
                    1
                    for r in self.results.values()
                    if isinstance(r, dict) and "error" in r
                ),
            },
            "detailed_results": self.results,
            "configuration": {
                "app_version": getattr(config, "app_version", "unknown"),
                "prediction_config": {
                    "model_base_path": config.prediction.model_base_path,
                    "default_hours": config.prediction.default_prediction_hours,
                    "max_hours": config.prediction.max_prediction_hours,
                },
            },
        }

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = (
            project_root
            / "logs"
            / f"prediction_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        report_file.parent.mkdir(exist_ok=True)

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # è¾“å‡ºæ‘˜è¦
        logger.info("=" * 60)
        logger.info("ğŸ¯ æµ‹è¯•æ‘˜è¦")
        logger.info("=" * 60)
        logger.info(f"æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}")
        logger.info(f"é€šè¿‡æµ‹è¯•: {report['summary']['passed_tests']}")
        logger.info(f"å¤±è´¥æµ‹è¯•: {report['summary']['failed_tests']}")

        success_rate = (
            report["summary"]["passed_tests"] / report["summary"]["total_tests"]
        ) * 100
        logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")

        if success_rate >= 80:
            logger.info("ğŸ‰ é¢„æµ‹ç³»ç»ŸéªŒè¯æˆåŠŸï¼")
        else:
            logger.warning("âš ï¸ é¢„æµ‹ç³»ç»Ÿå­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•")

        return report

    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹é¢„æµ‹ç³»ç»Ÿå®Œæ•´éªŒè¯...")

        # æµ‹è¯•åˆ—è¡¨
        tests = [
            ("æœåŠ¡åˆå§‹åŒ–", self.initialize_service),
            ("å¥åº·æ£€æŸ¥", self.test_service_health),
            ("æ¨¡å‹ä¿¡æ¯", self.test_model_info),
            ("QPSé¢„æµ‹", self.test_qps_prediction),
            ("CPUé¢„æµ‹", self.test_cpu_prediction),
            ("å†…å­˜é¢„æµ‹", self.test_memory_prediction),
            ("ç£ç›˜é¢„æµ‹", self.test_disk_prediction),
            ("å‚æ•°éªŒè¯", self.test_validation_errors),
            ("é…ç½®é›†æˆ", self.test_configuration_integration),
        ]

        start_time = time.time()

        for test_name, test_func in tests:
            try:
                logger.info(f"\n{'=' * 50}")
                logger.info(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
                logger.info(f"{'=' * 50}")

                success = await test_func()

                if success:
                    logger.info(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
                else:
                    logger.warning(f"âš ï¸ {test_name} æµ‹è¯•æœªå®Œå…¨é€šè¿‡")

            except Exception as e:
                logger.error(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {str(e)}")
                self.results[test_name.lower().replace(" ", "_")] = {"error": str(e)}

        duration = time.time() - start_time
        logger.info(f"\nâ±ï¸ æ€»æµ‹è¯•æ—¶é—´: {duration:.2f}ç§’")

        # ç”ŸæˆæŠ¥å‘Š
        report = await self.generate_report()

        return report


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ AI-CloudOps æ–°é¢„æµ‹ç³»ç»ŸéªŒè¯")
    logger.info("=" * 60)

    validator = PredictionSystemValidator()

    try:
        report = await validator.run_all_tests()

        # æ ¹æ®æµ‹è¯•ç»“æœé€€å‡º
        if report["summary"]["failed_tests"] == 0:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é¢„æµ‹ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
            sys.exit(0)
        else:
            logger.error(f"âŒ æœ‰ {report['summary']['failed_tests']} ä¸ªæµ‹è¯•å¤±è´¥")
            sys.exit(1)

    except Exception as e:
        logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
