#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AI-CloudOps-aiops
Author: Bamboo
Email: bamboocloudops@gmail.com
License: Apache 2.0
Description: AI-CloudOpsæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ç®¡ç†å™¨ - å¯é…ç½®çš„æç¤ºè¯ç³»ç»Ÿ
"""

import json
import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from app.models import PredictionType

logger = logging.getLogger("aiops.core.prediction.prompts")


@dataclass
class PromptTemplate:
    """æç¤ºè¯æ¨¡æ¿æ•°æ®ç±»"""

    name: str
    template: str
    variables: List[str]
    description: str
    category: str
    version: str = "1.0"

    def format(self, **kwargs) -> str:
        """æ ¼å¼åŒ–æ¨¡æ¿"""
        try:
            return self.template.format(**kwargs)
        except KeyError as e:
            raise ValueError(f"æ¨¡æ¿ {self.name} ç¼ºå°‘å¿…è¦å˜é‡: {e}")


class PromptTemplateManager:
    """æç¤ºè¯æ¨¡æ¿ç®¡ç†å™¨ - æ”¯æŒåŠ¨æ€åŠ è½½å’Œé…ç½®åŒ–ç®¡ç†"""

    def __init__(self):
        self.templates: Dict[str, PromptTemplate] = {}
        self._load_default_templates()

    def _load_default_templates(self) -> None:
        """åŠ è½½é»˜è®¤æç¤ºè¯æ¨¡æ¿"""

        # é¢„æµ‹åˆ†ææ¨¡æ¿
        self._register_template(
            PromptTemplate(
                name="prediction_analysis",
                category="analysis",
                description="åˆ†æå†å²æ•°æ®å’Œé¢„æµ‹ä¸Šä¸‹æ–‡",
                variables=[
                    "prediction_type",
                    "current_value",
                    "historical_data",
                    "time_context",
                ],
                template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº‘å¹³å°èµ„æºåˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹{prediction_type}é¢„æµ‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š

å½“å‰æŒ‡æ ‡å€¼: {current_value}
å†å²æ•°æ®: {historical_data}
æ—¶é—´ä¸Šä¸‹æ–‡: {time_context}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æï¼š
1. **æ•°æ®è´¨é‡è¯„ä¼°**: å†å²æ•°æ®çš„å®Œæ•´æ€§å’Œå¯é æ€§å¦‚ä½•ï¼Ÿ
2. **æ¨¡å¼è¯†åˆ«**: è¯†åˆ«å‡ºå“ªäº›æ—¶é—´æ¨¡å¼å’Œå‘¨æœŸæ€§å˜åŒ–ï¼Ÿ
3. **å½±å“å› ç´ **: å¯èƒ½å½±å“{prediction_type}å˜åŒ–çš„å…³é”®å› ç´ æœ‰å“ªäº›ï¼Ÿ
4. **é¢„æµ‹éš¾ç‚¹**: è¿™ç§ç±»å‹çš„é¢„æµ‹å¯èƒ½é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ
5. **å»ºè®®å…³æ³¨ç‚¹**: åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­åº”è¯¥é‡ç‚¹å…³æ³¨å“ªäº›æ–¹é¢ï¼Ÿ

è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€æä¾›åˆ†æç»“æœï¼Œæ¯ä¸ªç»´åº¦æ§åˆ¶åœ¨2-3å¥è¯å†…ã€‚""",
            )
        )

        # é¢„æµ‹ç»“æœè§£è¯»æ¨¡æ¿
        self._register_template(
            PromptTemplate(
                name="prediction_interpretation",
                category="interpretation",
                description="è§£è¯»å’Œåˆ†æé¢„æµ‹ç»“æœ",
                variables=[
                    "prediction_type",
                    "predictions_summary",
                    "confidence_stats",
                    "anomaly_info",
                    "trend_analysis",
                ],
                template="""ä½œä¸ºäº‘å¹³å°è¿ç»´ä¸“å®¶ï¼Œè¯·è§£è¯»ä»¥ä¸‹{prediction_type}é¢„æµ‹ç»“æœï¼š

é¢„æµ‹æ‘˜è¦: {predictions_summary}
ç½®ä¿¡åº¦ç»Ÿè®¡: {confidence_stats}
å¼‚å¸¸ä¿¡æ¯: {anomaly_info}
è¶‹åŠ¿åˆ†æ: {trend_analysis}

è¯·æä¾›ä¸“ä¸šçš„ç»“æœè§£è¯»ï¼š
1. **é¢„æµ‹è´¨é‡è¯„ä¼°**: æ ¹æ®ç½®ä¿¡åº¦å’Œè¶‹åŠ¿ï¼Œè¿™æ¬¡é¢„æµ‹çš„å¯é æ€§å¦‚ä½•ï¼Ÿ
2. **å…³é”®å‘ç°**: é¢„æµ‹ç»“æœä¸­æœ€é‡è¦çš„å‘ç°æ˜¯ä»€ä¹ˆï¼Ÿ
3. **é£é™©è¯†åˆ«**: é¢„æµ‹ç»“æœæ˜¾ç¤ºäº†å“ªäº›æ½œåœ¨é£é™©ï¼Ÿ
4. **æ—¶é—´èŠ‚ç‚¹**: éœ€è¦é‡ç‚¹å…³æ³¨çš„æ—¶é—´ç‚¹æœ‰å“ªäº›ï¼Ÿ
5. **èµ„æºå½±å“**: å¯¹å…¶ä»–èµ„æºå¯èƒ½äº§ç”Ÿçš„è¿é”å½±å“ï¼Ÿ

æ¯ä¸ªæ–¹é¢ç”¨1-2å¥ç®€æ´çš„ä¸“ä¸šè¯­è¨€æè¿°ã€‚""",
            )
        )

        # ç»¼åˆæŠ¥å‘Šæ¨¡æ¿
        self._register_template(
            PromptTemplate(
                name="comprehensive_report",
                category="report",
                description="ç”Ÿæˆç»¼åˆé¢„æµ‹åˆ†ææŠ¥å‘Š",
                variables=[
                    "prediction_type",
                    "analysis_context",
                    "prediction_results",
                    "scaling_recommendations",
                    "cost_analysis",
                    "insights",
                ],
                template="""ä½œä¸ºAIè¿ç»´ä¸“å®¶ï¼ŒåŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆ{prediction_type}é¢„æµ‹çš„ç»¼åˆåˆ†ææŠ¥å‘Šï¼š

åˆ†æèƒŒæ™¯: {analysis_context}
é¢„æµ‹ç»“æœ: {prediction_results}
æ‰©ç¼©å®¹å»ºè®®: {scaling_recommendations}
æˆæœ¬åˆ†æ: {cost_analysis}
ç³»ç»Ÿæ´å¯Ÿ: {insights}

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„ç»¼åˆæŠ¥å‘Šï¼ŒåŒ…å«ï¼š

## ğŸ“Š é¢„æµ‹æ¦‚è§ˆ
ç®€è¦æ€»ç»“é¢„æµ‹çš„æ ¸å¿ƒå‘ç°å’Œæ•´ä½“è¶‹åŠ¿

## âš ï¸ å…³é”®è­¦ç¤º
æ ‡è¯†éœ€è¦ç«‹å³æˆ–è¿‘æœŸå…³æ³¨çš„é£é™©ç‚¹

## ğŸ“ˆ è¶‹åŠ¿è§£è¯»
æ·±åº¦åˆ†æé¢„æµ‹è¶‹åŠ¿çš„ä¸šåŠ¡å«ä¹‰å’ŒæŠ€æœ¯å½±å“

## ğŸ’¡ ä¼˜åŒ–å»ºè®®
åŸºäºé¢„æµ‹ç»“æœæå‡ºçš„å…·ä½“ä¼˜åŒ–å»ºè®®

## ğŸ¯ è¡ŒåŠ¨è®¡åˆ’
çŸ­æœŸï¼ˆ24å°æ—¶å†…ï¼‰å’Œä¸­æœŸï¼ˆä¸€å‘¨å†…ï¼‰çš„æ¨èè¡ŒåŠ¨

æŠ¥å‘Šè¦æ±‚ï¼š
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…æŠ€æœ¯æœ¯è¯­è¿‡åº¦å¤æ‚
- çªå‡ºå¯æ“ä½œçš„å»ºè®®
- æ¯ä¸ªéƒ¨åˆ†æ§åˆ¶åœ¨3-4å¥è¯
- ä½¿ç”¨æ•°æ®æ”¯æ’‘è§‚ç‚¹""",
            )
        )

        # å¼‚å¸¸é¢„è­¦æ¨¡æ¿
        self._register_template(
            PromptTemplate(
                name="anomaly_alert",
                category="alert",
                description="å¼‚å¸¸æƒ…å†µé¢„è­¦åˆ†æ",
                variables=[
                    "prediction_type",
                    "anomaly_details",
                    "impact_assessment",
                    "historical_comparison",
                ],
                template="""æ£€æµ‹åˆ°{prediction_type}é¢„æµ‹ä¸­å­˜åœ¨å¼‚å¸¸æƒ…å†µï¼Œè¯·æä¾›ä¸“ä¸šé¢„è­¦åˆ†æï¼š

å¼‚å¸¸è¯¦æƒ…: {anomaly_details}
å½±å“è¯„ä¼°: {impact_assessment}
å†å²å¯¹æ¯”: {historical_comparison}

è¯·ç”Ÿæˆé¢„è­¦åˆ†æï¼š

ğŸš¨ **å¼‚å¸¸ç­‰çº§**: åŸºäºå½±å“èŒƒå›´å’Œä¸¥é‡ç¨‹åº¦åˆ¤æ–­å¼‚å¸¸ç­‰çº§

ğŸ” **æ ¹å› åˆ†æ**: åˆ†æå¯èƒ½å¯¼è‡´å¼‚å¸¸çš„åŸå› 

â° **æ—¶é—´çª—å£**: é¢„è®¡å¼‚å¸¸å½±å“çš„æ—¶é—´èŒƒå›´

ğŸ› ï¸ **åº”å¯¹ç­–ç•¥**: å»ºè®®çš„å¤„ç†å’Œç¼“è§£æªæ–½

ğŸ“Š **ç›‘æ§é‡ç‚¹**: éœ€è¦åŠ å¼ºç›‘æ§çš„æŒ‡æ ‡å’Œé˜ˆå€¼

ä¿æŒç®€æ´ä¸“ä¸šï¼Œé‡ç‚¹çªå‡ºå¯æ‰§è¡Œçš„å»ºè®®ã€‚""",
            )
        )

        # æ‰©ç¼©å®¹å†³ç­–æ¨¡æ¿
        self._register_template(
            PromptTemplate(
                name="scaling_decision",
                category="scaling",
                description="æ‰©ç¼©å®¹å†³ç­–åˆ†æ",
                variables=[
                    "prediction_type",
                    "current_resources",
                    "predicted_load",
                    "scaling_options",
                    "cost_considerations",
                ],
                template="""éœ€è¦ä¸º{prediction_type}åˆ¶å®šæ‰©ç¼©å®¹ç­–ç•¥ï¼Œè¯·åˆ†æä»¥ä¸‹ä¿¡æ¯ï¼š

å½“å‰èµ„æºé…ç½®: {current_resources}
é¢„æµ‹è´Ÿè½½: {predicted_load}
æ‰©ç¼©å®¹é€‰é¡¹: {scaling_options}
æˆæœ¬è€ƒé‡: {cost_considerations}

è¯·æä¾›æ‰©ç¼©å®¹å†³ç­–å»ºè®®ï¼š

## ğŸ¯ æ¨èæ–¹æ¡ˆ
åŸºäºé¢„æµ‹ç»“æœæ¨èçš„æœ€ä½³æ‰©ç¼©å®¹ç­–ç•¥

## âš–ï¸ æ–¹æ¡ˆæƒè¡¡
ä¸åŒæ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹å¯¹æ¯”åˆ†æ

## ğŸ’° æˆæœ¬æ•ˆç›Š
æ–¹æ¡ˆçš„æˆæœ¬æ•ˆç›Šåˆ†æå’ŒROIé¢„ä¼°

## â±ï¸ æ‰§è¡Œæ—¶æœº
å»ºè®®çš„æ‰§è¡Œæ—¶é—´å’Œåˆ†é˜¶æ®µå®æ–½è®¡åˆ’

## ğŸ”„ å›æ»šç­–ç•¥
å¦‚æœæ–¹æ¡ˆæ•ˆæœä¸ç†æƒ³çš„å¤‡é€‰æ–¹æ¡ˆ

æ¯ä¸ªéƒ¨åˆ†ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€ï¼Œçªå‡ºå†³ç­–ä¾æ®ã€‚""",
            )
        )

        # å¤šç»´åº¦é¢„æµ‹å¯¹æ¯”æ¨¡æ¿
        self._register_template(
            PromptTemplate(
                name="multi_dimension_comparison",
                category="comparison",
                description="å¤šç»´åº¦é¢„æµ‹ç»“æœå¯¹æ¯”åˆ†æ",
                variables=[
                    "prediction_results",
                    "correlation_analysis",
                    "resource_interaction",
                ],
                template="""è¯·åˆ†æå¤šä¸ªèµ„æºç»´åº¦çš„é¢„æµ‹ç»“æœåŠå…¶ç›¸äº’å…³ç³»ï¼š

é¢„æµ‹ç»“æœ: {prediction_results}
å…³è”æ€§åˆ†æ: {correlation_analysis}
èµ„æºäº¤äº’: {resource_interaction}

æä¾›å¤šç»´åº¦åˆ†æï¼š

## ğŸ”— èµ„æºå…³è”æ€§
åˆ†æä¸åŒèµ„æºæŒ‡æ ‡ä¹‹é—´çš„å…³è”å’Œç›¸äº’å½±å“

## âš ï¸ ç“¶é¢ˆè¯†åˆ«
è¯†åˆ«å¯èƒ½æˆä¸ºç³»ç»Ÿç“¶é¢ˆçš„èµ„æº

## ğŸ¨ ä¼˜åŒ–ç­–ç•¥
åŸºäºå¤šç»´åº¦åˆ†æçš„æ•´ä½“ä¼˜åŒ–å»ºè®®

## ğŸ“Š åè°ƒé…ç½®
å„èµ„æºç»´åº¦çš„åè°ƒé…ç½®å»ºè®®

ä¿æŒåˆ†æçš„ç³»ç»Ÿæ€§å’Œå®ç”¨æ€§ã€‚""",
            )
        )

        logger.info(f"å·²åŠ è½½ {len(self.templates)} ä¸ªé»˜è®¤æç¤ºè¯æ¨¡æ¿")

    def _register_template(self, template: PromptTemplate) -> None:
        """æ³¨å†Œæ¨¡æ¿"""
        self.templates[template.name] = template
        logger.debug(f"æ³¨å†Œæ¨¡æ¿: {template.name} ({template.category})")

    def get_template(self, name: str) -> Optional[PromptTemplate]:
        """è·å–æŒ‡å®šåç§°çš„æ¨¡æ¿"""
        return self.templates.get(name)

    def get_templates_by_category(self, category: str) -> List[PromptTemplate]:
        """è·å–æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰æ¨¡æ¿"""
        return [t for t in self.templates.values() if t.category == category]

    def list_templates(self) -> Dict[str, str]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡æ¿"""
        return {name: template.description for name, template in self.templates.items()}

    def format_template(self, template_name: str, **kwargs) -> str:
        """æ ¼å¼åŒ–æŒ‡å®šæ¨¡æ¿"""
        template = self.get_template(template_name)
        if not template:
            raise ValueError(f"æœªæ‰¾åˆ°æ¨¡æ¿: {template_name}")

        return template.format(**kwargs)

    def add_custom_template(self, template: PromptTemplate) -> None:
        """æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿"""
        self._register_template(template)
        logger.info(f"æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿: {template.name}")

    def load_templates_from_file(self, file_path: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½æ¨¡æ¿"""
        try:
            path = Path(file_path)
            if not path.exists():
                logger.warning(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return

            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)

            for template_data in data.get("templates", []):
                template = PromptTemplate(**template_data)
                self._register_template(template)

            logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(data.get('templates', []))} ä¸ªæ¨¡æ¿")

        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {str(e)}")

    def save_templates_to_file(self, file_path: str) -> None:
        """ä¿å­˜æ¨¡æ¿åˆ°æ–‡ä»¶"""
        try:
            templates_data = {
                "templates": [
                    {
                        "name": t.name,
                        "template": t.template,
                        "variables": t.variables,
                        "description": t.description,
                        "category": t.category,
                        "version": t.version,
                    }
                    for t in self.templates.values()
                ],
                "exported_at": datetime.now().isoformat(),
            }

            path = Path(file_path)
            path.parent.mkdir(parents=True, exist_ok=True)

            with open(path, "w", encoding="utf-8") as f:
                json.dump(templates_data, f, ensure_ascii=False, indent=2)

            logger.info(f"å·²ä¿å­˜ {len(self.templates)} ä¸ªæ¨¡æ¿åˆ°: {file_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {str(e)}")


class PredictionPromptBuilder:
    """é¢„æµ‹æç¤ºè¯æ„å»ºå™¨ - æ™ºèƒ½æ„å»ºé€‚åˆç‰¹å®šé¢„æµ‹åœºæ™¯çš„æç¤ºè¯"""

    def __init__(self, template_manager: PromptTemplateManager):
        self.template_manager = template_manager

    def build_analysis_prompt(
        self,
        prediction_type: PredictionType,
        current_value: float,
        historical_data: List[Dict],
        additional_context: Optional[Dict] = None,
    ) -> str:
        """æ„å»ºé¢„æµ‹åˆ†ææç¤ºè¯"""

        # å¤„ç†å†å²æ•°æ®æ‘˜è¦
        if historical_data:
            # å®‰å…¨è·å–å†å²æ•°æ®å€¼ï¼Œé˜²æ­¢ç±»å‹é”™è¯¯
            recent_values = []
            for d in historical_data[-24:]:  # æœ€è¿‘24ä¸ªæ•°æ®ç‚¹
                if isinstance(d, dict):
                    recent_values.append(d.get("value", 0))
                else:
                    recent_values.append(0)

            if recent_values:
                historical_summary = f"æœ€è¿‘æ•°æ®ç‚¹: {len(historical_data)}ä¸ª, æœ€è¿‘24ç‚¹èŒƒå›´: {min(recent_values):.2f}-{max(recent_values):.2f}"
            else:
                historical_summary = (
                    f"æœ€è¿‘æ•°æ®ç‚¹: {len(historical_data)}ä¸ª, æ•°æ®æ ¼å¼å¼‚å¸¸"
                )
        else:
            historical_summary = "æš‚æ— å†å²æ•°æ®"

        # ç”Ÿæˆæ—¶é—´ä¸Šä¸‹æ–‡
        now = datetime.now()
        time_context = f"å½“å‰æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M')}, å·¥ä½œæ—¥: {'æ˜¯' if now.weekday() < 5 else 'å¦'}, å·¥ä½œæ—¶é—´: {'æ˜¯' if 9 <= now.hour <= 18 else 'å¦'}"

        return self.template_manager.format_template(
            "prediction_analysis",
            prediction_type=prediction_type.value,
            current_value=current_value,
            historical_data=historical_summary,
            time_context=time_context,
        )

    def build_interpretation_prompt(
        self,
        prediction_type: PredictionType,
        prediction_results: Dict[str, Any],
        additional_analysis: Optional[Dict] = None,
    ) -> str:
        """æ„å»ºé¢„æµ‹ç»“æœè§£è¯»æç¤ºè¯"""

        # å®‰å…¨æ„å»ºé¢„æµ‹æ‘˜è¦ï¼Œé˜²æ­¢ç±»å‹é”™è¯¯
        if isinstance(prediction_results, dict):
            predictions = prediction_results.get("predicted_data", [])
            if predictions:
                # å®‰å…¨è·å–é¢„æµ‹å€¼
                values = []
                for p in predictions:
                    if isinstance(p, dict) and "predicted_value" in p:
                        values.append(p["predicted_value"])

                if values:
                    predictions_summary = f"é¢„æµ‹ç‚¹æ•°: {len(predictions)}, å€¼èŒƒå›´: {min(values):.2f}-{max(values):.2f}, å¹³å‡å€¼: {sum(values) / len(values):.2f}"
                else:
                    predictions_summary = f"é¢„æµ‹ç‚¹æ•°: {len(predictions)}, æ•°æ®æ ¼å¼å¼‚å¸¸"
            else:
                predictions_summary = "æš‚æ— é¢„æµ‹æ•°æ®"
        else:
            predictions_summary = "é¢„æµ‹ç»“æœæ ¼å¼å¼‚å¸¸"

        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidence_stats = "æš‚æ— ç½®ä¿¡åº¦ä¿¡æ¯"
        if isinstance(prediction_results, dict):
            predictions = prediction_results.get("predicted_data", [])
            if predictions:
                confidences = []
                for p in predictions:
                    if isinstance(p, dict) and p.get("confidence_level"):
                        confidences.append(p.get("confidence_level", 0))

                if confidences:
                    confidence_stats = f"å¹³å‡ç½®ä¿¡åº¦: {sum(confidences) / len(confidences):.2f}, èŒƒå›´: {min(confidences):.2f}-{max(confidences):.2f}"

        # å¼‚å¸¸ä¿¡æ¯
        anomaly_info = "æœªæ£€æµ‹åˆ°å¼‚å¸¸"  # é»˜è®¤å€¼
        if isinstance(prediction_results, dict):
            anomalies = prediction_results.get("anomaly_predictions", [])
            if anomalies:
                # å®‰å…¨è·å–é«˜é£é™©å¼‚å¸¸
                high_risk = []
                for a in anomalies:
                    if isinstance(a, dict) and a.get("impact_level") in [
                        "high",
                        "critical",
                    ]:
                        high_risk.append(a)
                anomaly_info = (
                    f"æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸ç‚¹ï¼Œå…¶ä¸­ {len(high_risk)} ä¸ªé«˜é£é™©"
                )
            else:
                anomaly_info = "æœªæ£€æµ‹åˆ°å¼‚å¸¸"

        # è¶‹åŠ¿åˆ†æ
        trend_analysis = "æ— è¶‹åŠ¿åˆ†ææ•°æ®"  # é»˜è®¤å€¼
        if isinstance(prediction_results, dict):
            trend_info = prediction_results.get("prediction_summary", {})
            if isinstance(trend_info, dict):
                trend_analysis = f"è¶‹åŠ¿: {trend_info.get('trend', 'æœªçŸ¥')}, å³°å€¼æ—¶é—´: {trend_info.get('peak_time', 'æœªçŸ¥')}"
            else:
                trend_analysis = "è¶‹åŠ¿åˆ†ææ•°æ®æ ¼å¼å¼‚å¸¸"

        return self.template_manager.format_template(
            "prediction_interpretation",
            prediction_type=prediction_type.value,
            predictions_summary=predictions_summary,
            confidence_stats=confidence_stats,
            anomaly_info=anomaly_info,
            trend_analysis=trend_analysis,
        )

    def build_comprehensive_report_prompt(
        self,
        prediction_type: PredictionType,
        analysis_context: str,
        prediction_results: Dict[str, Any],
        scaling_recommendations: List[Dict],
        cost_analysis: Optional[Dict] = None,
        insights: Optional[List[str]] = None,
    ) -> str:
        """æ„å»ºç»¼åˆæŠ¥å‘Šæç¤ºè¯"""

        # å¤„ç†æ‰©ç¼©å®¹å»ºè®®
        if scaling_recommendations:
            # å®‰å…¨è·å–åŠ¨ä½œä¿¡æ¯ï¼Œé˜²æ­¢ç±»å‹é”™è¯¯
            actions = []
            for r in scaling_recommendations:
                if isinstance(r, dict):
                    actions.append(r.get("action", "unknown"))
                else:
                    actions.append("unknown")
            scaling_summary = (
                f"å»ºè®® {len(scaling_recommendations)} é¡¹æ“ä½œ: {', '.join(set(actions))}"
            )
        else:
            scaling_summary = "æš‚æ— æ‰©ç¼©å®¹å»ºè®®"

        # å¤„ç†æˆæœ¬åˆ†æ
        if cost_analysis and isinstance(cost_analysis, dict):
            cost_summary = f"æˆæœ¬èŠ‚çœæ½œåŠ›: {cost_analysis.get('cost_savings_potential', 'æœªçŸ¥')}%, å½“å‰æˆæœ¬: {cost_analysis.get('current_hourly_cost', 'æœªçŸ¥')}/å°æ—¶"
        else:
            cost_summary = "æš‚æ— æˆæœ¬åˆ†æ"

        # å¤„ç†æ´å¯Ÿ
        insights_text = "; ".join(insights) if insights else "æš‚æ— ç‰¹æ®Šæ´å¯Ÿ"

        return self.template_manager.format_template(
            "comprehensive_report",
            prediction_type=prediction_type.value,
            analysis_context=analysis_context,
            prediction_results=str(
                prediction_results.get("prediction_summary", {})
                if isinstance(prediction_results, dict)
                else {}
            ),
            scaling_recommendations=scaling_summary,
            cost_analysis=cost_summary,
            insights=insights_text,
        )


# å…¨å±€æ¨¡æ¿ç®¡ç†å™¨å®ä¾‹
template_manager = PromptTemplateManager()
prompt_builder = PredictionPromptBuilder(template_manager)
