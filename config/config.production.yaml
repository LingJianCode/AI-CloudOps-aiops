# 应用基础配置
app:
  debug: false # 是否开启调试模式
  host: 0.0.0.0
  port: 8080
  log_level: WARNING

# Prometheus配置
prometheus:
  host: prometheus:9090
  timeout: 30

# LLM模型配置
llm:
  provider: openai # 可选值: openai, ollama
  model: Qwen/Qwen3-14B # 主模型
  task_model: Qwen/Qwen2.5-14B-Instruct # 任务模型
  temperature: 0.7 # LLM模型温度
  max_tokens: 2048 # LLM模型最大生成长度
  request_timeout: 15 # LLM请求超时时间(秒)
  # Ollama模型配置
  ollama_model: qwen2.5:3b
  ollama_base_url: http://ollama:11434

# 测试配置
testing:
  skip_llm_tests: false # 设置为true可跳过依赖LLM的测试

# Kubernetes配置
kubernetes:
  in_cluster: false # 是否使用Kubernetes集群内配置
  config_path: deploy/kubernetes/config # Kubernetes集群配置文件路径
  namespace: default

# 根因分析配置
rca:
  default_time_range: 30 # 默认时间范围(分钟)
  max_time_range: 1440 # 最大时间范围(分钟)
  anomaly_threshold: 0.65  # 根因分析异常阈值
  correlation_threshold: 0.7 # 根因分析相关度阈值
  default_metrics: # 默认监控指标
    - container_cpu_usage_seconds_total
    - container_memory_working_set_bytes
    - kube_pod_container_status_restarts_total
    - kube_pod_status_phase
    - node_cpu_seconds_total
    - node_memory_MemFree_bytes
    - kubelet_http_requests_duration_seconds_count
    - kubelet_http_requests_duration_seconds_sum

# 预测配置
prediction:
  model_path: data/models/time_qps_auto_scaling_model.pkl # 预测模型路径
  scaler_path: data/models/time_qps_auto_scaling_scaler.pkl # 预测模型缩放器路径
  max_instances: 20 # 预测模型最大实例数
  min_instances: 1 # 预测模型最小实例数
  prometheus_query: 'rate(nginx_ingress_controller_nginx_process_requests_total{service="ingress-nginx-controller-metrics"}[10m])' # 预测模型查询

# 通知配置
notification:
  enabled: true # 是否启用通知

# Redis配置 - 用于向量数据缓存和元数据存储
redis:
  host: redis
  port: 6379
  db: 0
  password: "v6SxhWHyZC7S"
  connection_timeout: 5 # Redis连接超时时间(秒)
  socket_timeout: 5 # RedisSocket超时时间(秒)
  max_connections: 10 # Redis最大连接数
  decode_responses: true # 是否解码响应

# 小助手配置
rag:
  vector_db_path: data/vector_db # 向量数据库路径
  collection_name: aiops-assistant # 向量数据库集合名称
  knowledge_base_path: data/knowledge_base # 知识库路径
  chunk_size: 1000 # 文档分块大小
  chunk_overlap: 200 # 文档分块重叠大小
  top_k: 4 # 最多返回的相似度
  similarity_threshold: 0.7 # 相似度阈值
  openai_embedding_model: Pro/BAAI/bge-m3 # OpenAI嵌入模型
  ollama_embedding_model: nomic-embed-text # Ollama嵌入模型
  max_context_length: 4000 # 最大上下文长度
  temperature: 0.1 # LLM模型温度
  cache_expiry: 3600 # 缓存过期时间(秒)
  max_docs_per_query: 8 # 每次查询最多处理的文档数
  use_enhanced_retrieval: true # 是否使用增强检索
  use_document_compressor: true # 是否使用文档压缩

# MCP配置
mcp:
  server_url: "http://mcp:9000" # MCP服务端地址
  timeout: 30 # 请求超时时间(秒)
  max_retries: 3 # 最大重试次数
  health_check_interval: 5 # 健康检查间隔(秒)
  